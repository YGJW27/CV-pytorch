{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MaxAbsScaler, KBinsDiscretizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"D:/code/DTI_data/network_FN/\"\n",
    "\n",
    "def data_list(sample_path):\n",
    "    sub_dirs = [x[0] for x in os.walk(sample_path)]\n",
    "    sub_dirs.pop(0)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)\n",
    "        file_glob = os.path.join(sample_path, dir_name, '*')\n",
    "        file_list.extend(glob.glob(file_glob))\n",
    "\n",
    "        for file_name in file_list:\n",
    "            data_list.append([file_name, dir_name])\n",
    "\n",
    "    return np.array(data_list)\n",
    "\n",
    "\n",
    "class MRI_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, target = self.data_list[idx][0], int(self.data_list[idx][1])\n",
    "        dataframe = pd.read_csv(filepath, sep=\"\\s+\", header=None)\n",
    "        pic = dataframe.to_numpy()\n",
    "\n",
    "        return pic, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    \n",
    "filelist = data_list(DATA_PATH)\n",
    "dataset = MRI_Dataset(filelist)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "for data, target, idx in dataloader:\n",
    "    x = data.numpy()\n",
    "    y = target.numpy()\n",
    "    idx = idx.numpy()\n",
    "    \n",
    "x = x.reshape(x.shape[0], -1)\n",
    "x = x[:, np.any(x, axis=0)]\n",
    "\n",
    "def Shannon_entropy(A):\n",
    "    unique, counts = np.unique(A, return_counts=True)\n",
    "    p = counts/counts.sum()\n",
    "    ent = -np.sum(p * np.log2(p))\n",
    "    return ent\n",
    "\n",
    "def mutual_information(A, B):\n",
    "    H_A = Shannon_entropy(A)\n",
    "    unique, counts= np.unique(B, return_counts=True)\n",
    "    H_A1B = 0\n",
    "    for idx, status in enumerate(unique):\n",
    "        H_A1B += Shannon_entropy(A[B==status]) * counts[idx]/counts.sum()\n",
    "    MI_AB = H_A - H_A1B\n",
    "    return MI_AB\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# seed = 1\n",
    "# cv = 20\n",
    "# kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "# acc_sum = 0\n",
    "# for idx, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "#     x_t = x[train_idx]\n",
    "#     y_t = y[train_idx]\n",
    "    \n",
    "#     est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "#     est.fit(x_t)\n",
    "#     x_d = est.transform(x_t)\n",
    "\n",
    "#     MI_array = np.zeros(x_d.shape[1])\n",
    "#     for i, e in enumerate(MI_array):\n",
    "#         MI_array[i] = Shannon_entropy(x_d[:, i])\n",
    "    \n",
    "#     x_d = x[:, MI_array>2.8]\n",
    "\n",
    "#     ynew_train = y[train_idx]\n",
    "#     ynew_test = y[test_idx]\n",
    "\n",
    "\n",
    "#     # Norm\n",
    "#     scaler = MaxAbsScaler()\n",
    "#     scaler.fit(x_d[train_idx])\n",
    "#     xnew_train = scaler.transform(x_d[train_idx])\n",
    "#     xnew_test = scaler.transform(x_d[test_idx])\n",
    "#     print(xnew_train.shape[1])\n",
    "    \n",
    "\n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf', random_state=1, gamma=0.01, C=10)\n",
    "#     model = svc.fit(xnew_train, ynew_train)\n",
    "\n",
    "#     predict = model.predict(xnew_test)\n",
    "#     correct = np.sum(predict == ynew_test)\n",
    "#     accuracy = correct / test_idx.size\n",
    "#     print(\"cv: {}/{}, acc.: {:.1f}\\n\".format(idx, cv, accuracy*100))\n",
    "#     acc_sum += accuracy\n",
    "# print(\"total acc.: {:.1f}\\n\".format(acc_sum / cv * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "node_path = 'D:/code/DTI_data/network_distance/AAL_90.node'\n",
    "nodes = pd.read_csv(node_path, sep=' ', header=None)\n",
    "nodes = nodes.iloc[:, 0:3]\n",
    "avg = torch.mean(data, axis=0).numpy()\n",
    "G = nx.from_numpy_array(avg)\n",
    "pos = dict(zip(range(nodes.shape[0]), [list(row) for row in nodes.to_numpy()]))\n",
    "nx.set_node_attributes(G, pos, 'coord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cfflib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b2a955667a28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcfflib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmyConnectome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnectome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmyConnectome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shear\\.conda\\envs\\torch1.2\\lib\\site-packages\\cfflib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mloadsave\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcfflib2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cfflib2'"
     ]
    }
   ],
   "source": [
    "from cfflib import *\n",
    "\n",
    "myConnectome = connectome()\n",
    "myConnectome.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `pts.mlab_source.dataset.lines` not found.\n"
     ]
    }
   ],
   "source": [
    "from mayavi import mlab\n",
    "\n",
    "def draw_network(G):\n",
    "    mlab.clf()\n",
    "    pos = np.array([pos for key, pos in G.nodes('coord')])\n",
    "    pts = mlab.points3d(pos[:, 0], pos[:, 1], pos[:, 2], resolution=20, scale_factor=5)\n",
    "    pts.mlab_source.dataset.lines = np.array(G.edges())\n",
    "    tube = mlab.pipeline.tube(pts,  tube_radius=0.5)\n",
    "    mlab.pipeline.surface(tube)\n",
    "\n",
    "    mlab.show()\n",
    "\n",
    "\n",
    "draw_network(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
